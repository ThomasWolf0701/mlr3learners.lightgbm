---
title: "mlr3learners.lightgbm: Multiclass Classification Example"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    keep_md: true
vignette: >
  %\VignetteIndexEntry{mlr3learners_lightgbm_multiclass}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
dir.create("./png")
```

```{r setup}
library(mlr3)
library(mlr3learners.lightgbm)
```

# Install the lightgbm R package

Before you can use the `mlr3learners.lightgbm` package, you need to install the lightgbm R package according to [its documentation](https://github.com/microsoft/LightGBM/blob/master/R-package/README.md) (this is necessary since lightgbm is neither on CRAN nor installable via `devtools::install_github`).

# Load the dataset

Start with creating a data set. The data must be provided as a `data.table` object. To simplify the subsequent steps, the target column name is assigned to the variable `target_col`.

```{r}
data("iris")
dataset <- data.table::as.data.table(iris)
target_col <- "Species"
```

To have independent validation data and test data, we further create a list `split`, containing the respective row indices.

```{r}
set.seed(17)
split <- list(
  train_index = sample(seq_len(nrow(dataset)), size = 0.7 * nrow(dataset))
)
temp_split <- setdiff(seq_len(nrow(dataset)), split$train_index)
set.seed(17)
split$valid_index <- sample(temp_split, size = 0.5 * length(temp_split))
split$test_index <- setdiff(temp_split, split$valid_index)
                            
table(dataset[split$train_index, target_col, with = F])
table(dataset[split$valid_index, target_col, with = F])
table(dataset[split$test_index, target_col, with = F])
```

# Define an mlr3 task 

We now can define an mlr3 training task.

```{r}
task <- TaskClassif$new(
  id = "iris",
  target = target_col,
  backend = dataset
)
```
```{r eval=FALSE}
mlr3viz::autoplot(task)
```

# Instantiate the lightgbm learner 

Then, the `classif.lightgbm` class needs to be instantiated: 

```{r}
learner <- mlr3::lrn("classif.lightgbm")
```

# Configure the learner 

In the next step, some of the learner's parameters need to be set. E.g., the parameters `nrounds` and `early_stopping_rounds` can be set here. Please refer to the [LightGBM manual](https://lightgbm.readthedocs.io) for further details these parameters. Almost all possible parameters have been implemented here. You can inspect them using the following command: 

```{r eval=FALSE}
learner$param_set
```

```{r}
learner$early_stopping_rounds <- 100
learner$nrounds <- 5000

learner$param_set$values <- list(
  "objective" = "multiclass",
  "learning_rate" = 0.01,
  "seed" = 17L
)
```

In order to use `early_stopping_rounds`, at least one validation set needs to be provided. We can do this by providing the task and the object `split$valid_index` to the learner's `valids` function.

```{r}
learner$valids(task, row_ids = split$valid_index)
learner$valid_data$dim()
```

# Train the learner 

The learner is now ready to be trained by using its `train` function. 

```{r results='hide', message=FALSE, warning=FALSE, error=FALSE}
learner$train(task, row_ids = split$train_index)
```

# Evaluate the model performance 

Basic metrics can be assesed directly from the learner model: 

```{r}
learner$model$best_iter
learner$model$best_score
```

The learner's `predict` function returns an object of mlr3's class `PredictionClassif`. 

```{r}
predictions <- learner$predict(task, row_ids = split$test_index)
head(predictions$response)
```

The predictions object includes also a confusion matrix:

```{r}
predictions$confusion
```

Further metrics can be calculated by using mlr3 measures:

```{r}
predictions$score(mlr3::msr("classif.logloss"))
```

The variable importance plot can be calculated by using the learner's `importance` function:

```{r}
importance <- learner$importance()
importance
```
```{r}
importance2 <- learner$importance2()
importance2$importance
```
